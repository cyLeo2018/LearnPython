# 爬虫准备
 - 参考资料
    - python网络数据采集
    - 精通python爬虫框架scrapy
    - http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html
    - https://blog.csdn.net/c406495762/article/details/72858983
 - 前提知识
    - url
    - http
    - web前端 html css js
    - ajax
    - re xpath
    - xml

# 爬虫简介
# 爬虫特征
 - 自定义下载网页数据或者内容
 - 自动在网络上流窜
# 爬虫运作步骤
    1.下载信息
    2.提取正确的信息
    3.根据自定义规则跳转其他页面执行1、2步骤
# 爬虫的分类
    1.通用爬虫
    2.专用爬虫(聚焦爬虫)

#Python网络包简介
 - Python2.x
    urllib\urllib2\urllib3\httplib\httplib2\requests
 - Python3.x
    urllib
  
## urllib
  - urllib.request 打开读取url
  - urllib.error 包含urllib.request常见错误，可以用try捕获
  - urllib.parse 包含解析url的方法,将URL解析成组件，URL解析和URL引用
  - urllib.robotparser  解析robots.txt文件

### urllib.request 
> 用于访问url的扩展库
#### urllib.request.urlopen
语法：

    urllib.request.urlopen(url, data=None, [timeout,]*, cafile=None, capath=None, cadefault=False, context=None)
    1.url:打开URL，URL可以是字符串或Request对象
    2.data=None:发送给服务端的数据，默认None 字节流
    3.打开url使用HTTP/1.1,请求头包含Connection:close
    4.timeout:连接超时时间，单位秒，对HTTP\HTTPS\FTP有效
    5.context:描述SSL选
    6.cafile\capath:CA证书
    7.cadefault:忽略
返回值:

    urlopen返回对象http.client.HTTPResponse,可作为文本管理，可以使用以下方法处理
        geturl() 返回URL地址
        info()  返回页面meta信息，例如请求头
        getcode() 返回HTTP状态码

#### class urllib.request.Request()
>URL请求构造
语法：

    class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)
    url
    data 发送到服务器端的数据，字节流
    headers
    origin_req_host
    unverifiable
    method

- 网页编码问题解决
    - encode() 编码 : string编码为ASCII Bytes
    - decode() 解码 : ASCII Bytes解码为string
    - chardet 第三方包，需要安装，自动检测页面编码格式，可能有误差
    
- request data的使用
    - 访问网站的两种方法
        - get 
            - 利用参数给服务器传递信息
            - 参数为dict，使用parse编码
        - post
            - 常用
            - 将信息自动加密处理
            - 使用urllib.request.urlopen(data=) data参数
            - http请求头变更
                - Content-Type: application/x-www.form-urlencode
                - Content-Length:数据长度
            - urllib.parse.urlencode()
            
        

### urllib.parse
> 将URL解析成组件,URL解析和URL引用
#### URL解析
##### URL字符串拆分到组件
###### urllib.parse.urlparse
语法：

    urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)
    1.
返回值：
    
    返回6个元素组成的元组
    URL: scheme://netloc/path;parameters?query#fragment.
    scheme
    netloc
    path
    params
    query
    fragment
    username
    password
    hostname
    port
###### urllib.parse.parse_qs
###### urllib.parse.parse_qsl
###### urllib.parse.urlunparse
###### urllib.parse.urlsplit
###### urllib.parse.urlunsplit
###### urllib.parse.urljoin
###### urllib.parse.urldefrag
    
     
##### URL组件重组为URL字符串
###### urllib.parse.quote
###### urllib.parse.quote_plus
###### urllib.parse.quote_from_bytes
###### urllib.parse.unquote
###### urllib.parse.unquote_plus
###### urllib.parse.unquote_to_bytes
###### urllib.parse.urlencode

### urllib.error
 - URLError产生的原因：
    - 网络不通
    - 服务器连接失败
    - 找不到指定的服务器
    - OSError的子类
 - HTTPError
    - URLError的子类
 - HTTPError对应HTTP请求返回的错误码，返回400以上的错误码引发HTTPError
 - URLError对应网络错误，包括url错误
 - OSError-URLError-HTTPError
 
 - UserAgent
    - UserAgent 用户代理，简称UA，属于headers一部分，服务器通过UA来判断访问者身份，但是极易伪造
    - 常见的UA值，使用的时候直接复制粘贴，也可以用浏览器访问的时候抓包
    ```
    1.Android

    Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19
    Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30
    Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1

    2.Firefox

    Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0
    Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0

    3.Google Chrome

    Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36
    Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19

    4.iOS

    Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3
    Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3   
    ```
    
    - 设置UA的方式：
        - heads
        - add_heads

- ProxyHandler处理(代理服务器)
    - 使用代理IP是爬虫的常用手段
    - 获取代理服务器的地址
        - www.xicidaili.com
        - www.goubanjia.com
    - 隐藏访问的真实信息
    - 基本使用步骤：
        1.设置代理地址
        2.创建ProxyHandler
        3.创建Opener
        4.安装Opener
            

- cookie & session
    - HTTP协议为无状态，弥补这个问题采用的补充协议
    - cookie是服务器发给客户端(浏览器)的一段信息，session是保存在服务器上与之对应的另一半信息，用来记录用户信息
- cookie与session的区别
    - 存放位置不同，cookie在客户端，session在服务端
    - cookie在客户端，可修改，并不安全
    - cookie存放不重要的信息
    - session会保存在服务器上一定时间，会过期
    - 单个cookie保存数据不超过4K，很多浏览器限制一个站点最多保存20个
- session的存放位置
    - 存放在服务器端
    - 一般情况下，session是存放在内存中或者数据库中，WEB框架自动处理

- 使用cookie登录
    - 直接把cookie复制下来，手动放入请求头中
    - http模块包含关于cookie的模块
        - CookieJar
            - 管理存储cookie，向http请求中添加cookie
            - cookie存储在内存中，CookieJar实例回收后cookie将消失
        - FileCookieJar(filename, delayload=None, policy=None):
            - 使用文件管理cookie
            - filename是保存cookie的文件
        - MozillaCookieJar(filename, delayload=None, policy=None):  
            - 创建与mocilla浏览器cookie.txt兼容的FileCookieJar实例
        - LwpCookieJar(filename, delayload=None, policy=None):
            - 创建于libwww-perl标准兼容的Set-Cookie3格式的FileCookieJar实例
        - CookieJar--> FileCookieJar--> MozillaCookieJar & LwpCookieJar
    - 利用cookiejar访问人人网
        - 自动使用cookie登录
            - 打开登录页面后自动通过用户名和密码登录
            - 自动提取返回的cookie
            - 利用提取的cookie登录隐私页面
            
    - handler是Handler的实例
    - 创建handler后，使用opener打开，打开后再用对应的handler处理
    - cookie做为一个变量，可以打印出来
        - cookie的属性
            - name
            - value
            - domain
            - path
            - expires
            - size
            - http
    - cookie的保存-FileCookieJar
    - cookie的读取

- SSL
    - SSL证书：遵守SSL安全套接层协议的服务器证书(SercureSocketLayer)
    - 美国网景公司开发
    - CA(CertifacationAuthority)数字证书认证中心，发放、管理、废弃证书的第三方机构
    - 遇到不信任的SSL证书，需要单独处理
   
- js加密
    - 有的反爬虫策略采用js加密(通常是md5)
    - 经过加密，数据以密文传输
    - 加密方法一定是在浏览器端完成，也就是会把js代码暴露给使用者
    - 通过查看加密方法，可以模拟加密过程，进而破解

- ajax
    - 异步请求
    - 一定会有URL，请求方法，可能有数据
    - 一般使用json格式

# Requests-献给人类
    - HTTP for humans，更简洁友好
    - 继承了urllib的所有特征
    - 底层使用的是urllib3
    - 开源地址：https://github.com/requests/requests
    - 中文文档：http://docs.python-requests.org/zh_CN/latest/index.html
    - 安装 conda install requests
    - get请求
        - requests.get(url)
        - requests.request("get", url)
        - 可以带有headers和parmas参数
    - get返回内容